{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "import ipywidgets\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p1_frontal_df = pd.read_csv('../datasets/kor_multi_modal/prep_part1_frontal_face.csv')\n",
    "p2_frontal_df = pd.read_csv('../datasets/kor_multi_modal/prep_part2_frontal_face.csv')\n",
    "p3_frontal_df = pd.read_csv('../datasets/kor_multi_modal/prep_part3_frontal_face.csv')\n",
    "p4_frontal_df = pd.read_csv('../datasets/kor_multi_modal/prep_part4_frontal_face.csv')\n",
    "p5_frontal_df = pd.read_csv('../datasets/kor_multi_modal/prep_part5_frontal_face.csv')\n",
    "\n",
    "dataframe = [p1_frontal_df, p2_frontal_df, p3_frontal_df, p4_frontal_df, p5_frontal_df]\n",
    "p_frontal_df = pd.concat(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_frontal_df = p_frontal_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>face_rect</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>{'max_y': 809, 'max_x': 1137, 'min_x': 720, 'm...</td>\n",
       "      <td>part1/KETI_MULTIMODAL_0000000375/KETI_MULTIMOD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>{'max_y': 809, 'max_x': 1137, 'min_x': 720, 'm...</td>\n",
       "      <td>part1/KETI_MULTIMODAL_0000000375/KETI_MULTIMOD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                          face_rect  \\\n",
       "0  neutral  {'max_y': 809, 'max_x': 1137, 'min_x': 720, 'm...   \n",
       "1  neutral  {'max_y': 809, 'max_x': 1137, 'min_x': 720, 'm...   \n",
       "\n",
       "                                            img_path  \n",
       "0  part1/KETI_MULTIMODAL_0000000375/KETI_MULTIMOD...  \n",
       "1  part1/KETI_MULTIMODAL_0000000375/KETI_MULTIMOD...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_frontal_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_f(img_path):\n",
    "    return \"/\".join(img_path.split(\"/\")[-5:])\n",
    "\n",
    "img_pathes = p_frontal_df['img_path'].map(lambda x : prep_f(x))\n",
    "# p_frontal_df['img_path'][:100].map(lambda x : prep_f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_frontal_df['img_path'] = img_pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'part1/KETI_MULTIMODAL_0000000375/KETI_MULTIMODAL_0000000375/KETI_SHOT_0000123017/KM_0000000634.jpg'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_frontal_df['img_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH='../../../datasets/emotion/multi-modal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d29f52e0ad7412e990885b924a067a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='x', max=1157797), Output()), _dom_classes=('widget-interâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.f(x)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    img_path = p_frontal_df['img_path'][x]\n",
    "    img_path = BASE_PATH + \"/\" + img_path \n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    print(img_path)\n",
    "    face_rect_dict = ast.literal_eval(p_frontal_df['face_rect'][x])\n",
    "    max_y = face_rect_dict['max_y']\n",
    "    max_x = face_rect_dict['max_x']\n",
    "    min_y = face_rect_dict['min_y']\n",
    "    min_x = face_rect_dict['min_x']\n",
    "    cv2.rectangle(img, (min_x, min_y), (max_x, max_y), [0, 255, 0], 3)\n",
    "    print(p_frontal_df['emotion'][x])\n",
    "    plt.imshow(img)\n",
    "    \n",
    "ipywidgets.interact(f, x = ipywidgets.IntSlider(0, min=0, max=p_frontal_df.shape[0]-1, step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download.sh  part1\tpart2\t   part3      part4\t part5\r\n",
      "nohup.out    part1.zip\tpart2.zip  part3.zip  part4.zip  part5.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../../datasets/emotion/multi-modal/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rt-face-detect",
   "language": "python",
   "name": "rt-face-detect"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
